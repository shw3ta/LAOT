---
layout: post
title: Contents
---
<span class="newthought">Hello.</span> These notes{% include sidenote.html id='about-notes' note='These notes are **under construction**, and will be updated once every two weeks (being optimistic). [Why am I doing this?](about/blog)' %} are based on my reading of [this book](www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/). Here, I explore statistical theory of learning. I discovered this book while TAing for CS1390{% include sidenote.html id='courseinfo' note='*Introduction to Machine Learning*, course taught by [Subhashish Banerjee](https://www.cse.iitd.ac.in/~suban/) at [Ashoka University](https://www.ashoka.edu.in/) in Monsoon 2021.'%}. I write here in the interest of making the content more accessible to a lay-reader.


<!-- 
## Math refreshers

1. [Review of probability theory](preliminaries/probabilityreview): Probability distributions. Conditional probability. Random variables. Expectation. (*full credits to Stanford CS228*).

2. [Review of linear algebra](): Under construction. -->

## Preliminaries

1. [Introduction](preliminaries/introduction/): An attempt to build a formal notion of the task of learning.

2. [Empirical Risk Minimization](preliminaries/ERM): What is observable? To what extent is the observable representative of the whole? How does it affect learning?

3. [A Formal Model](preliminaries/PAC-Learning): The importance of picking representative training sets. PAC learning. Relaxing a few assumptions. 


